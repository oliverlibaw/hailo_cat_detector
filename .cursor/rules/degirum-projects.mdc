---
description: 
globs: 
alwaysApply: true
---
# Use these code examples provided by DeGirum to determine how to implement object detection on a Raspberry Pi 5 with a Hailo AI Kit accelerator using the Degirum library. 


import cv2

import numpy as np



def resize_image_to_given_shape(image_path, input_shape=(1, 224, 224, 3)):

    """

    Reads an image using OpenCV, resizes it with INTER_LINEAR interpolation, and ensures it matches the specified size.



    Args:

        image_path (str): Path to the input image.

        input_shape (tuple): Desired shape of the output array (batch_size, height, width, channels).



    Returns:

        np.ndarray: Image array of shape matching the input shape.

    """

    if len(input_shape) != 4 or input_shape[0] != 1 or input_shape[3] != 3:

        raise ValueError("Input shape must be in the format (1, height, width, 3).")

    

    # Read the image using OpenCV

    image = cv2.imread(image_path)

    

    if image is None:

        raise FileNotFoundError(f"Image at path '{image_path}' could not be loaded.")

    

    # Convert BGR to RGB

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    

    # Resize the image using INTER_LINEAR interpolation

    resized_image = cv2.resize(image, (input_shape[2], input_shape[1]), interpolation=cv2.INTER_LINEAR)

    

    if resized_image.shape != (input_shape[1], input_shape[2], input_shape[3]):

        raise ValueError(f"Resized image has an unexpected shape: {resized_image.shape}")

    

    # Expand dimensions to match the batch size

    return np.expand_dims(resized_image, axis=0)









import degirum

from pprint import pprint



# Load the model

model = dg.load_model(

    model_name='mobilenet_v2_1.0',

    inference_host_address='@local',

    zoo_url='<path_to_model_zoo>'

)



# Prepare the input image

image_array = resize_image_to_given_shape('<path_to_cat_image>', model.input_shape[0])



# Run inference

inference_result = model(image_array)



# Pretty print the results

pprint(inference_result.results)









# Interpreting the results



[{'data': array([[[[0, 0, 0, ..., 0, 0, 0]]]], dtype=uint8),

  'id': 0,

  'name': 'mobilenet_v2_1_0/softmax1',

  'quantization': {'axis': -1, 'scale': [0.003921568859368563], 'zero': [0]},

  'shape': [1, 1, 1, 1001],

  'size': 1001,

  'type': 'DG_UINT8'}]





# Post-processing



import numpy as np

import json



def postprocess_classification_output(output, labels_file, topk=5):

    """

    Postprocesses the model output to extract top-k predictions, maps them to labels,

    and adjusts class indices based on label length compatibility.



    Args:

        output (list): The raw model output containing quantized data and metadata.

        labels_file (str): Path to the JSON file containing the list of labels.

        topk (int): Number of top predictions to extract.



    Returns:

        list of dict: List of dictionaries containing adjusted_class_id, class_id, labels, and probabilities.

    """

    # Load labels from the JSON file

    with open(labels_file, "r") as f:

        labels = json.load(f)  



    # Extract the first output (assuming only one output is present)

    output_data = output[0]



    # Extract relevant fields

    data = output_data['data']  # Quantized data

    scale = output_data['quantization']['scale'][0]  # Quantization scale

    zero = output_data['quantization']['zero'][0]  # Quantization zero point



    # Dequantize the data

    dequantized_data = (data.astype(np.float32) - zero) * scale



    # Flatten the data (assumes shape [1, 1, 1, N])

    dequantized_data = dequantized_data.flatten()



    # Get the top-k indices and probabilities

    top_k_indices = np.argsort(dequantized_data)[-topk:][::-1]  # Indices of top-k predictions

    top_k_probs = dequantized_data[top_k_indices]  # Probabilities of top-k predictions



    # Determine if class_index should be adjusted

    if len(labels) == len(dequantized_data):

        subtract_one = False

    elif len(labels) == len(dequantized_data) - 1:

        subtract_one = True

    else:

        print(f"Warning: Labels file is not compatible with output results. "

              f"Labels length: {len(labels)}, Output length: {len(dequantized_data)}")

        return []    



    # Process the results and map to labels

    processed_results = []

    for class_index, probability in zip(top_k_indices, top_k_probs):

        if subtract_one:

            # Adjust class_index if needed

            adjusted_class_index = class_index - 1

            if class_index == 0:

                label = "Background"  # Background class exists only when subtract_one is True

            elif 0 <= adjusted_class_index < len(labels):

                label = labels[str(adjusted_class_index)]

            else:

                label = "Unknown"

        else:

            # No adjustment needed for class_index

            adjusted_class_index = class_index

            if 0 <= adjusted_class_index < len(labels):

                label = labels[str(adjusted_class_index)]

            else:

                label = "Unknown"



        processed_results.append({

            "category_id": adjusted_class_index,

            "label": label,

            "score": probability

        })



    return processed_results





# Using the function to get the top 5 predictions



top5_predictions = postprocess_classification_output(inference_result.results, '<path_to_labels_file>', topk=5)

pprint(top5_predictions)



# Example of JSON with built-in pre-processor and post-processor



{

  "ConfigVersion": 10,

  "Checksum": "d6c4d0b9620dc2e5e215dfab366510a740fe86bf2c5d9bd2059a6ba3fe62ee63",

  "DEVICE": [

    {

      "DeviceType": "HAILO8",

      "RuntimeAgent": "HAILORT",

      "SupportedDeviceTypes": "HAILORT/HAILO8"

    }

  ],

  "PRE_PROCESS": [

    {

      "InputType": "Image",

      "ImageBackend": "opencv",

      "InputPadMethod": "stretch",

      "InputResizeMethod": "bilinear",

      "InputN": 1,

      "InputH": 224,

      "InputW": 224,

      "InputC": 3,

      "InputQuantEn": true

    }

  ],

  "MODEL_PARAMETERS": [

    {

      "ModelPath": "mobilenet_v2_1.0.hef"

    }

  ],

  "POST_PROCESS": [

    {

      "OutputPostprocessType": "Classification",

      "OutputTopK": 5,

      "OutputNumClasses": 1000,

      "OutputClassIDAdjustment": 1,

      "LabelsPath": "labels_ILSVRC2012_1000.json"

    }

  ]

}



# Example utility function for image handling:



import cv2



def read_image_as_rgb(image_path):

    # Load the image in BGR format (default in OpenCV)

    image_bgr = cv2.imread(image_path)

    

    # Check if the image was loaded successfully

    if image_bgr is None:

        raise ValueError(f"Error: Unable to load image from path: {image_path}")

    

    # Convert the image from BGR to RGB

    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    

    return image_rgb





# Function to print the height and width of an image



import cv2



def print_image_size(image_path):

    # Load the image

    image = cv2.imread(image_path)

    

    # Check if the image was loaded successfully

    if image is None:

        print(f"Error: Unable to load image from path: {image_path}")

    else:

        # Get the image size (height, width, channels)

        height, width, channels = image.shape

        print(f"Image size: {height}x{width} (Height x Width)")





# Function to display an RGB image array



import matplotlib.pyplot as plt

def display_images(images, title="Images", figsize=(15, 5)):

    """

    Display a list of images in a single row using Matplotlib.



    Parameters:

    - images (list): List of images (NumPy arrays) to display.

    - title (str): Title for the plot.

    - figsize (tuple): Size of the figure.

    """

    num_images = len(images)

    fig, axes = plt.subplots(1, num_images, figsize=figsize)

    if num_images == 1:

        axes = [axes]  # Make it iterable for a single image

    for ax, image in zip(axes, images):

        ax.imshow(image)

        ax.axis('off')

    fig.suptitle(title, fontsize=16)

    plt.tight_layout()

    plt.show()





# Example of resize with letterbox



import cv2

import numpy as np



def resize_with_letterbox(image_path, target_shape, padding_value=(0, 0, 0)):

    """

    Resizes an image with letterboxing to fit the target size, preserving aspect ratio.

    

    Parameters:

        image_path (str): Path to the input image.

        target_shape (tuple): Target shape in NHWC format (batch_size, target_height, target_width, channels).

        padding_value (tuple): RGB values for padding (default is black padding).

        

    Returns:

        letterboxed_image (ndarray): The resized image with letterboxing.

        scale (float): Scaling ratio applied to the original image.

        pad_top (int): Padding applied to the top.

        pad_left (int): Padding applied to the left.

    """

    # Load the image from the given path

    image = cv2.imread(image_path)

    

    # Check if the image was loaded successfully

    if image is None:

        raise ValueError(f"Error: Unable to load image from path: {image_path}")

    

    # Convert the image from BGR to RGB

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    

    # Get the original image dimensions (height, width, channels)

    h, w, c = image.shape

    

    # Extract target height and width from target_shape (NHWC format)

    target_height, target_width = target_shape[1], target_shape[2]

    

    # Calculate the scaling factors for width and height

    scale_x = target_width / w

    scale_y = target_height / h

    

    # Choose the smaller scale factor to preserve the aspect ratio

    scale = min(scale_x, scale_y)

    

    # Calculate the new dimensions based on the scaling factor

    new_w = int(w * scale)

    new_h = int(h * scale)



    # Resize the image to the new dimensions

    resized_image = cv2.resize(image, (new_w, new_h),interpolation=cv2.INTER_LINEAR)

    

    # Create a new image with the target size, filled with the padding value

    letterboxed_image = np.full((target_height, target_width, c), padding_value, dtype=np.uint8)

    

    # Compute the position where the resized image should be placed (padding)

    pad_top = (target_height - new_h) // 2

    pad_left = (target_width - new_w) // 2

    

    # Place the resized image onto the letterbox background

    letterboxed_image[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = resized_image



    final_image = np.expand_dims(letterboxed_image, axis=0)

    

    # Return the letterboxed image, scaling ratio, and padding (top, left)

    return final_image, scale, pad_top, pad_left





# example of preprocessing the image



image_array, scale, pad_top, pad_left = resize_with_letterbox('<path to cat image>', (1, 640,640,3))

display_images([image_array[0]])



#example of running the model predict function



import degirum

from pprint import pprint



# Load the model

model = dg.load_model(

    model_name='yolo11n',

    inference_host_address='@local',

    zoo_url='<path_to_model_zoo>'

)



# Prepare the input image

image_array, scale, pad_top, pad_left = resize_with_letterbox('<path_to_cat_image>', model.input_shape[0])



# Run inference

inference_result = model(image_array)



# Pretty print the results

pprint(inference_result.results)





# example of displaying output visualization



import cv2

import numpy as np



def overlay_bboxes_and_labels(image, annotations, color=(0, 255, 0), font_scale=1, thickness=2):

    """

    Overlays bounding boxes and labels on the image for a list of annotations.

    

    Parameters:

        image (ndarray): The input image (in RGB format).

        annotations (list of dicts): List of dictionaries with 'bbox' (x1, y1, x2, y2) and 'label' keys.

        color (tuple): The color of the bounding box and text (default is green).

        font_scale (int): The font scale for the label (default is 1).

        thickness (int): The thickness of the bounding box and text (default is 2).

    

    Returns:

        image_with_bboxes (ndarray): The image with the bounding boxes and labels overlayed.

    """

    # Convert the image from RGB to BGR (OpenCV uses BGR by default)

    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

    

    # Loop over each annotation (bbox and label)

    for annotation in annotations:

        bbox = annotation['bbox']  # Bounding box as (x1, y1, x2, y2)

        label = annotation['label']  # Label text

        

        # Unpack bounding box coordinates

        x1, y1, x2, y2 = bbox

        

        # Convert float coordinates to integers

        x1, y1, x2, y2 = int(round(x1)), int(round(y1)), int(round(x2)), int(round(y2))

        

        # Draw the rectangle (bounding box)

        cv2.rectangle(image_bgr, (x1, y1), (x2, y2), color, thickness)

        

        # Put the label text on the image

        cv2.putText(image_bgr, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)

    

    # Convert the image back to RGB for display or further processing

    image_with_bboxes = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    

    return image_with_bboxes





# Scaling the results to the original image



def reverse_rescale_bboxes(annotations, scale, pad_top, pad_left, original_shape):

    """

    Reverse rescales bounding boxes from the letterbox image to the original image, returning new annotations.



    Parameters:

        annotations (list of dicts): List of dictionaries, each containing a 'bbox' (x1, y1, x2, y2) and other fields.

        scale (float): The scale factor used for resizing the image.

        pad_top (int): The padding added to the top of the image.

        pad_left (int): The padding added to the left of the image.

        original_shape (tuple): The shape (height, width) of the original image before resizing.



    Returns:

        new_annotations (list of dicts): New annotations with rescaled bounding boxes adjusted back to the original image.

    """

    orig_h, orig_w = original_shape  # original image height and width

    

    new_annotations = []

    

    for annotation in annotations:

        bbox = annotation['bbox']  # Bounding box as (x1, y1, x2, y2)

        

        # Reverse padding

        x1, y1, x2, y2 = bbox

        x1 -= pad_left

        y1 -= pad_top

        x2 -= pad_left

        y2 -= pad_top

        

        # Reverse scaling

        x1 = int(x1 / scale)

        y1 = int(y1 / scale)

        x2 = int(x2 / scale)

        y2 = int(y2 / scale)

        

        # Clip the bounding box to make sure it fits within the original image dimensions

        x1 = max(0, min(x1, orig_w))

        y1 = max(0, min(y1, orig_h))

        x2 = max(0, min(x2, orig_w))

        y2 = max(0, min(y2, orig_h))

        

        # Create a new annotation with the rescaled bounding box and the original label

        new_annotation = annotation.copy()

        new_annotation['bbox'] = (x1, y1, x2, y2)

        

        # Append the new annotation to the list

        new_annotations.append(new_annotation)

    

    return new_annotations